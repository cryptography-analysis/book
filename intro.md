# Wstęp teoretyczny
W tym rozdziale przedstawiamy krótki przegląd historycznego rozwoju szyfrów oraz technik łamania kodów lub kryptologii. Pomimo pojawienia się nowoczesnych komputerów, wiele z tych klasycznych szyfrów pozostaje nierozwiązanych lub nie ma efektywnej metody kryptoanalizy dla nich. Przedstawiamy bardziej nowoczesne zastosowanie technik optymalizacyjnych, a konkretniej metaheurystyk lokalnego przeszukiwania, do kryptoanalizy klasycznych szyfrów. Opisujemy również ich ograniczenia i wyzwania związane z projektowaniem efektywnych technik. Następnie przedstawiamy przegląd wkładu tej pracy dyplomowej. Wkład obejmuje nowatorską, efektywną metodologię, zweryfikowaną na przykładach, która umożliwia kryptoanalizę kilku rodzajów trudnych szyfrów, odszyfrowanie historycznych wiadomości, które wcześniej nie mogły być odczytane, oraz rozwiązanie kilku wiodących historycznych wyzwań szyfrowych. Kończymy opisem struktury tej pracy dyplomowej.

# 1.1 Klasyczna kryptografia
Użycie tajnych kodów lub szyfrów do bezpiecznej transmisji poufnych wiadomości, zarówno dyplomatycznych, wojskowych, jak i osobistych, zostało zarejestrowane już w starożytnej Grecji. Techniki łamania kodów, czyli kryptologii, rozwijały się równolegle do rozwoju kryptografii. Wcześniejsze szyfry obejmowały proste szyfry substytucyjne, np. szyfr Cezara i szyfr substytucji jednoalfabetycznej. Najskuteczniejsze ataki na klasyczne szyfry, oprócz zdobycia kluczy, były przede wszystkim natury statystycznej. Metoda używania analizy częstotliwości do rozwiązania szyfrów substytucji jednoalfabetycznej została wprowadzona przez A-Kindi, arabskiego polimatę, w IX wieku. W wyniku tych rozwojów twórcy nowych klasycznych szyfrów zrozumieli, że aby zwiększyć kryptograficzną bezpieczeństwo szyfru, należy dokładać starań, aby ukryć rozpoznawalne statystyczne wzorce w tekstach zaszyfrowanych. Aby przezwyciężyć ograniczenia pojedynczej (jednoalfabetycznej) substytucji, nowe szyfry używały kilku alfabetów substytucji (substytucja polialfabetyczna), takich jak szyfr Vigenère. Ten szyfr przez 300 lat uchodził za "Le Chiffre indechiffrable", czyli nie do złamania. Ten szyfr został rozwiązany dopiero w XIX wieku, gdy rozwinięto bardziej zaawansowane metody statystyczne, opracowane przez Babage'a i Kasisky'ego. Metody te obejmowały analizę statystyczną par liter (digramów) lub trójek liter (trigramów) oraz badanie powtórzeń. Bardziej zaawansowane wersje szyfrów substytucyjnych, takie jak szyfry homofoniczne lub Playfair, oraz nowe szyfry transpozycyjne, zostały wprowadzone przez armie i w celu komunikacji dyplomatycznej. Szyfry transpozycyjne, takie jak szyfr transpozycji kolumnowej, wprowadzono, aby ukryć statystyki digramów lub trigramów, poprzez przemieszczanie liter tekstu jawnego

Innym sposobem ukrycia statystyk językowych było zastosowanie złożonego szyfru, z kilkoma etapami, takiego jak podwójny szyfr transpozycyjny lub kombinowane szyfry substytucji i transpozycji, takie jak szyfr ADFGVX. Oba szyfry zostały wprowadzone podczas I wojny światowej. Innymi sposobami uniemożliwienia kryptoanalitykowi odkrycia przydatnych wzorców statystycznych było ograniczenie długości każdej wiadomości oraz ograniczenie liczby wiadomości korzystających z tego samego klucza, aby uniknąć tzw. "głębokości". Kryptoanaliza nowych szyfrów okazała się jeszcze bardziej wymagająca i wymagała bardziej zaawansowanych metod statystycznych. W większości przypadków te szyfry nie mogły być złamane za wyjątkiem specjalnych przypadków lub gdy dostępna była duża ilość ruchu. Okres międzywojenny przyniósł również przejście kryptoanalizy z nauki wymagającej głównie umiejętności językowych na naukę wymagającą umiejętności matematycznych. Opracowano nowsze metody statystyczne, w tym słynny indeks zbieżności Williama Friedmana. Aby przezwyciężyć ograniczenia ręcznych szyfrów, pojawiły się maszyny szyfrujące. Maszyny szyfrujące opracowane w latach 20., 30. i 40. XX wieku były głównie mechanicznego lub elektromechanicznego projektu i wykorzystywały obroty lub obracające się rotory jako mechanizm szyfrowania. Istniały dwa główne typy mechanizmu rotacyjnego dla maszyn szyfrujących. Pierwszy typ obejmował niemiecką maszynę Enigma i brytyjską maszynę Typex, a ich rotory wdrażały serię kroków substytucji. Mechanizm obracających się rotorów został zaprojektowany w taki sposób, że alfabet substytucji zmienia się dla każdego znaku, a sekwencja substytucji nie powtarza się co najmniej kilkanaście tysięcy do milionów cykli. Tworzy to polialfabetyczny system substytucji z ogromną liczbą alfabetów. Inną, ale podobną wariantą tych maszyn były japońskie maszyny Red i Purple, które miały przełączniki obracające się zamiast obracających się rotorów. 

Drugim typem maszyn szyfrujących były urządzenia Hagelina oraz telegraficzne urządzenia szyfrujące Lorenz SZ40/42. Urządzenia te również posiadały rotory krokowe, jednak ich rotory miały pinezki, które kontrolowały generowanie pseudolosowego ciągu klucza dodawanego do tekstu jawnego w celu wygenerowania tekstu zaszyfrowanego. Urządzenia te próbowały naśladować działanie szyfru strumieniowego z kryptograficznym jednorazowym kluczem. Wprowadzenie maszyn szyfrujących w latach 20. i 30. XX wieku spowodowało rozwój innych maszyn wykorzystywanych do kryptoanalizy, co efektywnie stworzyło "wojnę maszyn przeciwko maszynom". Mimo że wiele szyfrów było wciąż rozwiązywanych ręcznie do czasów II wojny światowej i nawet po niej, to maszyny, takie jak maszyny tabulacyjne IBM i urządzenia takie jak polski Bombe, zaczęły odgrywać kluczową rolę w kryptoanalizie, kulminując w opracowaniu maszyny Turinga i pierwszego w pełni elektronicznego, dużego, programowalnego systemu komputerowego, Colossusa. W latach 60. XX wieku pojawiły się w pełni elektroniczne urządzenia szyfrujące, a w latach 70. XX wieku szyfrowanie oparte na komputerach lub układach scalonych, najważniejszym wydarzeniem było wprowadzenie standardu szyfrowania danych (DES). Wraz z pojawieniem się kryptografii klucza publicznego te rozwinięcia oznaczały koniec ery klasycznej kryptografii.

# 1.2 Rozwój kryptologii dla szyfrów klasycznych

Rozległa literatura z początku XIX i XX wieku, a także ostatnio zdeklasyfikowany materiał NSA, dostarczają bogatej informacji na temat manualnych metod kryptoanalitycznych. Szczegóły na temat historycznych metod można znaleźć w publikacjach. Szczegóły dotyczące maszyn łamiących szyfry rozwijanych przed i w czasie II wojny światowej, w tym Turing Bombe i Colossus, zostały opublikowane, a w muzeach można obejrzeć w pełni funkcjonalne repliki tych maszyn. Jasne jest, że agencje kryptograficzne, takie jak NSA, od lat 50. XX wieku szeroko wykorzystywały komputery ogólnego przeznaczenia, ale od 1945 roku opadła zasłona tajemnicy na dostępne źródła.

Dostępne są niewielkie źródła dotyczące tych zagadnień po roku 1945. Niektóre ostatnio zdeklasyfikowane materiały NSA przedstawiają równoległy rozwój informatyki w przemyśle i świecie akademickim od czasów II wojny światowej do lat 70. XX wieku, wraz z coraz większym wykorzystaniem technologii komputerowej przez NSA. NSA często określała wymagania dla najnowszych technologii i systemów komputerowych. Niestety, prawie nie ma informacji na temat ich wykorzystania komputerów do rozwiązywania konkretnych systemów szyfrowania, a jeszcze mniej na temat szczegółów komputerowej kryptologii szyfrów.

W historii kryptografii można wyróżnić kilka przykładów, gdzie głównym celem kryptologów było zidentyfikowanie szczegółów systemu lub maszyny szyfrującej. Jest to dość łatwe, jeśli maszyna lub instrukcja szyfrowania trafią w ręce osoby próbującej odczytać dany szyfr, ale istnieje wiele przykładów skomplikowanych systemów, które zostały zidentyfikowane i zanalizowane tylko na podstawie przechwyconych transmisji, takich jak japoński system Purple przez USA czy system Lorenz SZ42 przez Brytyjczyków. Zasada Kerckhoffs’a, sformułowana w XIX wieku i sformułowana ponownie przez Claude’a Shannona w 1949 roku, określa, że system szyfrujący powinien być bezpieczny nawet wtedy, gdy trafi w ręce wroga. Ponadto, kanały transmisji powinny być uważane za nieszyfrowane, a założenie jest takie, że wrogi jest w stanie przechwycić zaszyfrowane komunikaty. Bezpieczeństwo systemu szyfrującego powinno więc opierać się na niemożności odzyskania kluczy szyfrowania z przechwyconych zaszyfrowanych transmisji przez wroga, a nie na braku wiedzy o systemie szyfrowania.

Metody kryptoanalizy atakujące szyfry można podzielić na następujące kategorie:
    
   • Ataki wyłącznie na podstawie tekstu szyfrogramu: Jest to najbardziej ogólny rodzaj ataku. Dostępny jest tylko tekst szyfrogramu oraz wiedza o systemie szyfrowania, ale nie o konkretym kluczu używanym do szyfrowania tekstu jawnego.
   
   • Ataki na podstawie znanego tekstu jawnego: W niektórych przypadkach znana jest część tekstu jawnego lub cały tekst jasny, oprócz odpowiadającego mu tekstu szyfrogramu. W takim przypadku warto próbować odzyskać pierwotny klucz szyfrowania, aby odczytać inne wiadomości zaszyfrowane tym samym lub podobnym kluczem. Ten rodzaj ataku jest wykonalny, na przykład, gdy używane są stereotypowe początki lub końcówki lub gdy tekst jasny wiadomości został uzyskany za pomocą innych metod.
   
   • Ataki na podstawie wybranego tekstu jawnego: W tym przypadku atakujący nie tylko zna tekst jasny, ale faktycznie wybiera tekst lub teksty jasne, które zostaną zaszyfrowane i przesłane jako szyfrogram. Metoda ta jest często jedyną dostępną do atakowania nowoczesnych szyfrów i wykracza poza zakres naszych badań, ponieważ nie była istotna dla klasycznych szyfrów.

Dla każdego rodzaju szyfru istnieje teoretycznie możliwość ataku brutalnej siły. Jedynym wyjątkiem jest szyfr jednorazowego użycia, ponieważ nawet jeśli przetestujemy wszystkie możliwe klucze, nie ma sposobu na określenie, który z tekstów jawnego (oraz kluczy użytych do deszyfrowania szyfrogramu) jest prawidłowy. Dla innych szyfrów, najprostszą metodą odzyskania klucza szyfrowania jest wyczerpująca próba odszyfrowania zaszyfrowanego tekstu wszystkimi możliwymi kluczami. Ten rodzaj ataku jest łatwy do przeprowadzenia dla bardzo prostych szyfrów, takich jak szyfr Cezara, ale dla większości klasycznych szyfrów przestrzeń kluczy jest po prostu zbyt duża, przynajmniej dla ręcznego ataku metodą brutalnej siły. Od lat 30-tych XX wieku, ataki brute-force były czasami wykonywane przy użyciu maszyn lub generowanych przez nie katalogów, na przykład w kryptologii systemów Enigmy bez panelek połączeniowych. Wykonalność ataków brutalnej siły musi często być ponownie oceniana podczas rozważania nowych technologii, takich jak współczesne komputery. Ataki brutalnej siły nie są jednak skupieniem tej pracy, ani też szyfry, które są podatne na tego rodzaju ataki.

Podczas radzenia sobie z kluczami o bardzo dużej przestrzeni możliwości często stosowanym podejściem jest podejście "dzielenia i zwyciężania". W takim podejściu kryptolog stara się odzyskać część lub części klucza, jednocześnie ignorując inne części tego samego klucza. Takie podejście było często stosowane do kryptoanalizy systemów z wieloma etapami szyfrowania, takich jak Enigma. W naszych badaniach często będziemy polegać na takim podejściu, zwłaszcza w przypadku niektórych bardziej wymagających szyfrów. Podejścia "dzielenia i zwyciężania" czasami mogą być łączone z "pół-brute-force'owym atakiem" na niektóre części klucza, jednocześnie ignorując inne.

Metody kryptanalizy można również podzielić ze względu na wykorzystywaną technologię. W przypadku metod manualnych, proces kryptanalityczny, w tym możliwie żmudna analiza statystyczna, wykonywana jest ręcznie, używając tylko długopisu i papieru. Była to jedyna opcja do łamania szyfrów podczas I wojny światowej. Metody manualne były wciąż w powszechnym użyciu podczas II wojny światowej, a następnie, do czasu pojawienia się komputerów. Kryptanaliści często uważali manualną kryptanalizę szyfrów nie tylko za naukę, ale również za sztukę, pozostawiając dużo miejsca dla intuicji i doświadczenia. Metody zmechanizowane, z wykorzystaniem maszyn mechanicznych lub elektromechanicznych, zostały opracowane w latach 30. XX wieku. Polska maszyna Bombe została opracowana do tworzenia katalogów "cyklicznych wzorców" szyfru Enigma. W czasie II wojny światowej zwiększyło się wykorzystanie maszyn IBM do zastąpienia stosunkowo prostych, ale uciążliwych i podatnych na błędy, procesów przetwarzania przez urzędników lub kryptanalityków. Bardziej zaawansowane maszyny, takie jak brytyjska Turing Bombe i Colossus, zostały zbudowane podczas II wojny światowej i zostały zaprojektowane od podstaw, aby wprowadzić procesy, które nie mogły być praktycznie wykonywane ręcznie. Ostatnią kategorią są metody komputerowe, wykorzystujące komputery ogólnego przeznaczenia, takie jak komputery główne, superkomputery, komputery osobiste i nawet rozproszone komputery oparte na chmurze. Komputery były używane przez agencje łamiące kody, takie jak NSA, już od lat 50. XX wieku, ale bardzo mało jest znane o ich użyciu i konkretnych metodach kryptanalitycznych do łamania konkretnych kodów. Począwszy od lat 80. XX wieku, szeroki zakres badań dotyczących komputerowej kryptanalizy klasycznych szyfrów jest dostępny w domenie publicznej.

# 1.3 Kryptoanaliza klasycznych szyfrów jako problem optymalizacyjny

Nadejście nowoczesnych komputerów otworzyło drzwi dla technik, które w przeszłości były zbyt uciążliwe do wykonywania ręcznie lub zbyt drogie, ponieważ wymagały projektowania i produkcji drogich maszyn takich jak Colossus i Enigma Bombe. W niektórych przypadkach możliwe jest teraz przeprowadzenie wyczerpującego przeszukania całego klucza za pomocą brutalnej siły, na przykład dla szyfru transpozycji z mniej niż 15 elementami. Jednak sama moc obliczeniowa komputera nie wystarcza do większości bardziej wymagających klasycznych systemów szyfrujących i maszyn. To było motywacją do zastosowania technik optymalizacji do kryptoanalizy klasycznych szyfrów, głównie opartych na metaheurystykach lokalnego przeszukiwania.

Najpierw opisujemy, dlaczego stosowanie metaheurystyk poszukiwania lokalnego jest istotne dla większości klasycznych szyfrów, podczas gdy nie ma to zastosowania dla większości nowoczesnych szyfrów. Pomimo ich coraz większej skomplikowania, klasyczne szyfry i maszyny szyfrujące nie są w stanie całkowicie ukryć wzorców statystycznych. Używając terminologii Shannona, klasyczne szyfry mają niskie lub ograniczone rozproszenie, to znaczy, są ograniczone w swojej zdolności do ukrywania wzorców statystycznych. Ponadto, jeśli kryptoanalityk zna prawie wszystkie (ale nie wszystkie) właściwe elementy klucza, odszyfrowanie szyfrogramu za pomocą prawie właściwego klucza zwykle produkuje prawie właściwy tekst jawnie. Ponadto klucz z mniejszą liczbą błędów prawdopodobnie wygeneruje tekst z mniejszą liczbą błędów. Jeśli liczba błędów przekroczy pewien poziom, odszyfrowany tekst może w ogóle nie być czytelny. Ale w niektórych przypadkach, nawet jeśli odszyfrowany tekst nie jest czytelny, może ujawnić bardzo subtelne cechy statystyczne, takie jak wartość indeksu koincydencji nieco wyższa niż dla sekwencji losowych liter.

Warto zauważyć, że nowoczesne systemy szyfrowania, takie jak DES czy AES, całkowicie ukrywają jakiekolwiek statystyczne wzorce i zostały zaprojektowane z wysokimi poziomami dyfuzji. Wystarczy, że jeden bit w kluczu będzie niepoprawny, aby wygenerować odszyfrowane teksty, które są statystycznie równoważne sekwencji czysto losowych symboli. Ponadto, nawet mała zmiana w tekście jawnym może prowadzić do drastycznych zmian w tekście zaszyfrowanym.

Zaproponowano i przebadano kilka metaheurystyk przeszukiwania lokalnego do kryptoanalizy klasycznych szyfrów, takich jak przeszukiwanie z tabu, algorytmy genetyczne, symulowane wyżarzanie. Inne metaheurystyki stosowane do kryptoanalizy klasycznych szyfrów obejmują algorytmy optymalizacji kolonii mrówek, algorytmy optymalizacji roju cząstek i Markov Chain Monte Carlo. W większości przypadków metaheurystyki przeszukiwania lokalnego były wdrożone w prosty sposób, z minimalnym dostosowaniem do konkretnego problemu. Niektóre prace porównują również kilka metaheurystyk równocześnie. W większości przypadków metaheurystyki są stosowane do prostych szyfrów podstawieniowych lub szyfrów transpozycyjnych z krótkimi kluczami, np. od 15 do 25 elementów, skupiając się na bardziej prostych przypadkach pełnych prostokątów transpozycji. Ogólnie rzecz biorąc, wcześniejsze studia przypadków nie wykazały znaczącej poprawy w porównaniu do tego, co można już było osiągnąć za pomocą metod manualnych.

W latach 90-tych dokonano przełomu dzięki bardziej złożonym algorytmom hill climbing, dostosowanym do kryptoanalizy konkretnych szyfrów. W 1995 roku Gillogly przedstawił pierwszą kryptoanalizę maszyny Enigma jedynie na podstawie tekstu zaszyfrowanego. W czasie II wojny światowej alianci mieli tylko rozwiązanie znanego tekstu jawnego, stosując Bombe Turinga. Metoda Gillogly'ego została udoskonalona przez Weieruda i Sullivana i zastosowana do skutecznego deszyfrowania setek oryginalnych niemieckich wiadomości wojskowych. Została również wykorzystana do deszyfrowania oryginalnych niemieckich wiadomości Enigmy 4-kołowej marynarki wojennej, przy użyciu obliczeń rozproszonych. Dostosowane algorytmy hill climbing zostały również skutecznie zastosowane do kryptoanalizy japońskiego szyfru Purple oraz brytyjskiego systemu Typex. Symulowane wyżarzanie z powodzeniem zostało zastosowane do kryptoanalizy krótkich szyfrów Playfair. W przeciwieństwie do wcześniejszych prac wykorzystujących metaheurystyki lokalnego przeszukiwania, wyniki były zdecydowanie lepsze niż to, co mogło być osiągnięte wcześniej za pomocą historycznych metod. Co ciekawe, jak dotąd algorytmy genetyczne, choć szeroko badane w kontekście klasycznych szyfrów, nie okazały się być lepsze od innych metaheurystyk dla żadnego z klasycznych szyfrów. W badaniach porównawczych, stwierdzono, że algorytmy genetyczne są porównywalne lub nie lepsze od symulowanego wyżarzania lub przeszukiwania z tabu w kontekście kryptoanalizy prostego szyfru podstawieniowego i szyfru przestawieniowego kolumnowego.

# 1.4 Wyzwania

W tej sekcji opisane są wyzwania związane z zastosowaniem metaheurystyk lokalnego przeszukiwania do kryptoanalizy klasycznych szyfrów.

Hill climbing (HC) jest jedną z najczęściej stosowanych metaheurystyk lokalnego przeszukiwania do kryptoanalizy klasycznych szyfrów. W przypadku klasycznych szyfrów, poprawienie klucza, aby poprawić większą liczbę elementów, zwykle skutkuje stopniowymi poprawami odszyfrowanego tekstu jawnego. Jest to w przeciwieństwie do nowoczesnych szyfrów o wysokiej dyfuzji, w których nawet jedno błędne rozwiązanie klucza całkowicie lub niemal całkowicie niszczy odszyfrowany tekst. Hill climbing został zaproponowany do zastosowania w informatycznej kryptoanalizie ręcznie tworzonych szyfrów, takich jak szyfry podstawieniowe i szyfry przestawieniowe, a także maszynowych szyfrów, takich jak Enigma. Hill climbing w swojej najbardziej ogólnej postaci jest prosty do zaimplementowania i w wielu przypadkach działa podobnie lub lepiej niż inne metaheurystyki lokalnego przeszukiwania, takie jak algorytmy genetyczne. Jednak Hill climbing ma swoje ograniczenia. W swojej ogólnej postaci często nie jest wystarczająco potężny do kryptoanalizy niektórych bardziej wymagających klasycznych szyfrów i maszyn szyfrujących, gdy używane są mocne i bezpieczne parametry.

Najbardziej znane ograniczenie Hill climbing polega na ryzyku utknięcia w lokalnym maksimum, zanim osiągnie się pożądane globalne maksimum. To ograniczenie często jest usuwane przy użyciu technik takich jak losowe restarty (patrz Sekcja 3.3), listy tabu lub symulowane wyżarzanie (SA) (Sekcja 3.4). SA jest w zasadzie wariantem HC, który również umożliwia niektóre transformacje w dół. Jednak istnieją również inne wymagania, aby algorytmy HC lub SA były skuteczne.

Dwa główne elementy w projektowaniu algorytmu Hill climbing to funkcja oceny dla kluczy kandydujących i transformacje klucza, umożliwiające przeszukiwanie nowych kluczy kandydujących w jego sąsiedztwie. Idealnie, funkcja oceny powinna osiągnąć maksymalną wartość dla poprawnego klucza. Powinna również być monotoniczna, czyli im bliższy klucz kandydujący jest do poprawnego klucza, tym wyższa powinna być ocena. Wymaganie monotonicznej funkcji oceny może być trudne do spełnienia (patrz Sekcja 3.2). Często mogą wystąpić obszary niemonotoniczne, czyli lepszy klucz (z mniejszą ilością błędów) dający gorszą ocenę. Takie przypadki mogą uniemożliwić Hill climbingowi zbieganie do poprawnego klucza. Ponadto, rozpoczynając od klucza kandydującego z wieloma błędami, poprawa jednego lub kilku elementów klucza może nie skutkować zauważalną poprawą funkcji oceny. Te przypadki prawdopodobnie uniemożliwią Hill climbingowi "start", a algorytm nie będzie w stanie dotrzeć do obszaru z wystarczająco silnym gradientem, z którego może zbiegać w kierunku maksimum. Może być konieczne użycie kilku funkcji oceny, takich jak szorstka początkowa funkcja oceny, wystarczająco wrażliwa, aby umożliwić algorytmowi opuszczenie obszaru niemonotonicznego lub obszaru z szumem, oraz bardziej selektywna funkcja oceny stosowana na późniejszych etapach, aby lepiej zbiegać, gdy pewna liczba elementów klucza została poprawnie odzyskana. Istnieje kilka ogólnych funkcji oceny powszechnie stosowanych do kryptoanalizy historycznych szyfrów, takich jak statystyki n-gramów, ale wybór odpowiedniej funkcji oceny lub projektowanie nowej jest zadaniem skomplikowanym, a to, co czyni funkcję oceny skuteczną, nie jest jasne a priori (patrz sekcja 4.5).

To nie są jedyne problemy związane z zastosowaniem prostego algorytmu HC do kryptoanalizy klasycznych szyfrów. Losowe wybranie początkowego klucza często nie wystarcza.

Takie początkowe klucze mogą być zbyt niskiej jakości, czyli zbyt głęboko w obszarze płaskim lub nie-monotonicznym funkcji oceny. Mogą być potrzebne bardziej wyszukane metody generowania dobrych początkowych kluczy. Transformacje używane do tworzenia nowych kluczy kandydujących w sąsiedztwie bieżącego klucza mogą być zbyt drastyczne, na przykład przez przetasowanie zbyt wielu elementów klucza z wcześniejszego etapu, a większość wiedzy zdobytej w poprzednich etapach poszukiwań może zostać utracona. Może być zbyt wiele transformacji do sprawdzenia przy każdej iteracji, co znacznie wydłuża czas poszukiwań. Z drugiej strony, sprawdzanie zbyt małej liczby transformacji może sprawić, że algorytm przegapi możliwości zbieżności. W innych przypadkach poszukiwania nie będą mogły postępować w kierunku poprawnego klucza, jeśli nie ma serii pojedynczych transformacji, które mogą prowadzić do coraz wyższych wyników i jednocześnie zbliżać się do ostatecznego, poprawnego klucza.

System szyfrowania może być zbyt złożony i wymagać podejścia dziel i rządź lub skomplikowanej sekwencji wielokrotnych lub zagnieżdżonych faz lokalnego przeszukiwania.

Kolejnym wyzwaniem związanym z kryptologią krótkich wiadomości jest minimalna długość, która gwarantuje statystyczną istotność wyników. Dla każdej metody punktacji istnieje minimalna długość wiadomości, poniżej której wyniki są zbyt szumne i nieistotne. Ta minimalna długość niezbędna do osiągnięcia znaczenia statystycznego jest zwykle większa niż Odległość Unicity (ang. Unicity Distance). Odległość Unicity to minimalna długość tekstu jawnego, która zapewnia, że szyfrogram uzyskany przez zaszyfrowanie tekstu jawnego P1 kluczem K1, nie może być uzyskany przez zaszyfrowanie innego tekstu jawnego P2 innym kluczem K2, z określonym prawdopodobieństwem. Poniżej pewnej długości tekstu jawnego algorytm poszukiwania klucza może generować fałszywe wyniki z wysokimi punktacjami, tj. złe klucze z wysokimi wynikami, które są wyższe niż wynik dla poprawnego klucza (patrz sekcja 3.2.4).

Kolejny zbiór wyzwań dotyczy trudności zastosowania jakichkolwiek technik kryptograficznych opartych na statystyce do niektórych bardziej bezpiecznych rodzajów klasycznych szyfrów lub gdy są one używane z bezpiecznymi parametrami. Pierwszym wyzwaniem jest wielkość przestrzeni klucza, która nie tylko (i oczywiście) wyklucza użycie technik brutalnej siły, ale także zwiększa złożoność dowolnego algorytmu wyszukiwania. Stopień trudności jest zwiększany w systemach szyfrujących z zmienną długością klucza, takich jak szyfr podwójnej transpozycji. Im dłuższy klucz, tym większa przestrzeń klucza. Ponadto, w dobrze zaprojektowanym szyfrze, z dobrą dyfuzją i gdy jest używany prawidłowo, może nie być możliwe uzyskanie jakiegokolwiek pomiaru statystycznego, który jest istotnie różny od tego samego pomiaru na losowym strumieniu symboli. Wyzwaniem dla kryptoanalityka jest zidentyfikowanie potencjalnego odchylenia od losowości, które może być wykryte za pomocą pewnego pomiaru statystycznego, oraz zaprojektowanie metody punktacji i transformacji klucza, aby wykorzystać to odchylenie. Ten problem jest intensyfikowany w przypadku krótszych kryptogramów, kiedy zmierzone odchylenie od losowości (w szyfrogramie odszyfrowanym za pomocą klucza kandydata) może wynikać z szumów statystycznych, a nie z bliskości klucza kandydata do prawidłowego klucza.

Jednym z wniosków wynikających z opisanych tutaj wyzwań jest to, że skuteczność ataku kryptograficznego musi być oceniana w kontekście kilku parametrów:

• Długość szyfrogramu: Długość szyfrogramu, lub łączna długość wszystkich szyfrogramów, jeśli jest dostępnych kilka szyfrogramów. W przypadku ataku znając-tekst-jawnego, odnosi się to do liczby znanych lub domyślnych symboli.

• Złożoność klucza: Dla niektórych szyfrów długość klucza może się różnić. Dłuższy klucz oznacza większą przestrzeń klucza i większą przestrzeń poszukiwań. Dla niektórych maszyn szyfrujących niektóre parametry mogą być bardziej bezpieczne niż inne, na przykład funkcja nakładania się zębów w maszynie szyfrującej Hagelin M-209 (im więcej nakładających się zębów, tym szyfr jest bardziej bezpieczny).

Czy chodzi Ci o określony dokument lub źródło, gdzie ta analiza może być znaleziona? Jeśli tak, proszę podać więcej informacji lub odwołanie.

Pomimo rozwoju nowoczesnych technik komputerowych oraz coraz większej dostępności zasobów obliczeniowych, nie opublikowano jeszcze żadnego rozwiązania komputerowego dla kilku ważnych klasycznych szyfrów i maszyn szyfrujących, takich jak szwajcarski NEMA, amerykańskie maszyny SIGABA i KL-7 oraz rosyjski Fialka. Dla innych rozwiązania są ograniczone pod względem wydajności i obejmują tylko najbardziej korzystne przypadki, takie jak dla maszyny Hagelina M-209, Hagelina CX-52 i szyfru podwójnej transpozycji. W przypadku wielu z nich nie ma opublikowanych skutecznych ataków szyfrogramów tylko z tekstu zaszyfrowanego, a także nie ma opublikowanych ataków z tekstem jawnym. Atak z tekstem jawnym jest zazwyczaj łatwiejszy do przeprowadzenia niż atak szyfrogramu tylko z tekstu zaszyfrowanego. Mimo że nowsze prace wykazały obiecujące wyniki i okazały się skuteczniejsze niż metody historyczne, to nie została jeszcze zaproponowana żadna ramowa lub formalna metodologia dotycząca tego, co czyni algorytm lokalnego przeszukiwania skutecznym w kryptografii klasycznych szyfrów. Wynikiem badań opisanych w tej pracy jest wszechstronna, formalizowana i uogólniona metodyka, silnie zweryfikowana w studiach przypadków, dla zastosowania metaheurystyk lokalnego przeszukiwania w celu skutecznej kryptanalizy trudnych klasycznych szyfrów.

# 1.5 Znaczenie pracy

Studium klasycznych szyfrów oraz ich kryptoanalizy ma wysoką wartość z kilku powodów. Posiada wartość historyczną, umożliwiającą lepsze zrozumienie ewolucji szyfrów i łamania kodów przez całą historię oraz ich często ukrytych ról w przebiegu wydarzeń historycznych. Ponadto, w niektórych przypadkach pewne historyczne wiadomości zachowały się tylko w postaci zaszyfrowanej i mogą zostać odczytane jedynie w przypadku udanej kryptoanalizy (łamania kodu) szyfru. Istnieje kilka przypadków, gdzie nowoczesne metody komputerowe pozwoliły na odszyfrowanie tych wcześniej niedostępnych dokumentów, takich jak odszyfrowanie oryginalnych wiadomości z Enigmy z czasów II wojny światowej i niemieckich wiadomości ADFGVX z czasów I wojny światowej opisanych w tej pracy.

W środowiskach edukacyjnych, takich jak programy akademickie z dziedziny informatyki lub szkoły średnie, nauka klasycznych szyfrów jest często kluczowa dla wywołania zainteresowania u uczniów kryptografią i informatyką ogólnie. W takich środowiskach zasady nowoczesnej kryptografii są często najlepiej wprowadzane i zrozumiałe w kontekście przykładów kryptografii klasycznej. Ponadto, kryptografia klasyczna oferuje więcej możliwości natychmiastowych nagród i zwiększonej motywacji w postaci ćwiczeń i wyzwań związanych z łamaniem szyfrów. Istnieje również możliwość wyciągnięcia pewnych wniosków z przeszłych niepowodzeń klasycznych szyfrów, takich jak zbyt duże poleganie na złożoności systemu kryptograficznego lub jego przestrzeni kluczy, zamiast dokładnego badania jego bezpieczeństwa kryptograficznego. To może być jeszcze bardziej istotne dzisiaj, ponieważ dwa główne elementy kryptografii klasycznej, czyli substytucja i transpozycja, są wciąż używane dzisiaj w różnych formach jako elementy składowe nowoczesnych szyfrów.

Badanie kryptoanalizy klasycznych szyfrów za pomocą nowoczesnych technik może również pomóc w zrozumieniu historycznych technik łamania kodów, ponieważ nowoczesne i historyczne metody często polegają na tych samych właściwościach statystycznych.

Mimo że klasyczne szyfry na pierwszy rzut oka wydają się łatwiejsze do kryptoanalizy niż nowoczesne szyfry, to wciąż istnieje wiele nierozwiązanych wyzwań historycznych oraz klasycznych szyfrów, dla których nie istnieją znane skuteczne metody kryptoanalizy.

# 1.6 Wkład

Pierwszym celem tej pracy badawczej jest zidentyfikowanie czynników algorytmów kryptoanalizy opartych na metaheurystykach lokalnego przeszukiwania, które czynią je skutecznymi lub nieskutecznymi, na podstawie analizy przypadków, niektórych udanych (patrz sekcje 3.5.1, 3.5.2 i 3.5.3), a niektórych mniej (sekcja 3.5.4). Drugim celem jest opracowanie nowej metodologii efektywnej kryptoanalizy oraz sformułowanie zasad i wytycznych dla projektowania skutecznych algorytmów opartych na metaheurystykach lokalnego przeszukiwania. Trzecim celem jest zweryfikowanie metodologii, stosując ją do serii trudnych problemów szyfrów klasycznych, w tym do rozwiązania wyzwań szyfrów i deszyfracji oryginalnych historycznych dokumentów zaszyfrowanych.

Główne wkłady tej pracy badawczej i rozprawy obejmują:

# 1.6.1 Wkład 1 – Nowa Metodologia Efektywnej Kryptoanalizy Szyfrów Klasycznych z Wykorzystaniem Metaheurystyk Lokalnego Przeszukiwania

Ta praca przedstawia nową metodologię opartą na pięciu głównych zasadach, które obejmują główne modyfikacje, rozszerzenia, adaptacje i inne zalecenia. Mają one na celu przekształcenie wspinaczki górskiej i symulowanego wyżarzania w bardzo skuteczne narzędzia do kryptoanalizy szyfrów klasycznych, zwłaszcza w przypadku trudnych problemów szyfrów i kryptoanalizy, gdy naiwne zastosowanie wspinaczki górskiej lub symulowanego wyżarzania nie przynosi wystarczających wyników. Pięć głównych zasad kierujących obejmuje:

Identyfikację najbardziej skutecznych metaheurystyk, w większości przypadków wspinaczkę górską (a dla określonych przypadków, symulowane wyżarzanie). Obejmuje to specjalne procesy wieloetapowe lub zagnieżdżone.

Ograniczenie przestrzeni kluczy poprzez klasyczne podejście "podziel i zwyciężaj" lub z użyciem innych metod.

Projektowanie i wybór skutecznych funkcji oceny, ściśle dostosowanych do konkretnego problemu lub do konkretnego etapu rozwiązania, które spełniają niezbędne kryteria dla dobrych wyników.

Skuteczne przekształcenia klucza, zapewniające dobrą pokrycie przestrzeni poszukiwań i równocześnie zapewniające stały postęp w kierunku rozwiązania.

Wielokrotne restarty z kluczami początkowymi wysokiej jakości.

Te zasady są szczegółowo przedstawione w rozdziale 4.

# 1.6.2 Wkład 2 – Nowe skuteczne ataki kryptograficzne na kilka trudnych klasycznych szyfrów lub ustawień szyfrów

Jako część studiów przypadków dla tej pracy badawczej, zastosowaliśmy metodologię do kryptoanalizy kilku klasycznych szyfrów, dla których obecnie nie istnieje znane rozwiązanie komputerowe lub rozwiązanie jest ograniczone w zakresie i wydajności, takie jak rozwiązanie zastosowalne tylko dla krótkich kluczy. Kryterium uwzględnienia klasycznego szyfru jako studium przypadku w tej pracy badawczej było zdolność do pokazania istotnego wpływu, poprzez umożliwienie rozwiązania szyfru, dla którego nie istnieje żadne rozwiązanie, lub znaczącej poprawy w porównaniu do istniejących rozwiązań. Dla każdego takiego szyfru zastosowaliśmy jedną lub więcej z zasad kierunkowych, a następnie oceniliśmy wydajność, w tym mierzenie wskaźnika sukcesu w różnych warunkach, porównanie z poprzednimi metodami oraz ocenę czynnika pracy.

Osiągnięto znaczące ulepszenia dla następujących problemów kryptoanalizy szyfrów:

• Szyfr podwójnej transpozycji z użyciem kluczy dłuższych niż 15 elementów.

• Szyfr pojedynczej transpozycji z długimi kluczami, o długości co najmniej 25 elementów.

• Szyfr pojedynczej transpozycji z niekompletnymi prostokątami transpozycji.

• Szyfr ADFGVX z użyciem kluczy transpozycji dłuższych niż 15 elementów.

• Maszyna szyfrująca M-209, atak z tekstem jawnym na krótkie wiadomości.

• Maszyna szyfrująca M-209, atak tylko na podstawie tekstu zaszyfrowanego.

• Chaocipher, atak z tekstem jawnym.

• Chaocipher, atak tylko na podstawie tekstu zaszyfrowanego w głębi.

• Enigma, odzyskanie ustawień klucza na podstawie niewielkiej liczby podwójnych wskaźników (artykuł do przedłożenia).

Te studia przypadków są przedstawione w rozdziałach 5, 6, 7, 8, 9 i 10. Większość tych nowych i wydajnych metod kryptoanalizy jest obecnie stanem sztuki w dziedzinie kryptoanalizy konkretnych klasycznych szyfrów.

# 1.6.3 Wkład 3 - Odszyfrowanie dokumentów historycznych i rozwiązania wyzwań kryptograficznych

Niektóre z przypadków zakończyły się odniesieniem sukcesu w odszyfrowaniu oryginalnych historycznych kryptogramów lub rozwiązaniu publicznych wyzwań kryptograficznych, w tym:

• Po raz pierwszy od 1918 roku udało się odszyfrować kolekcję 600 oryginalnych niemieckich wiadomości wojennych zaszyfrowanych szyfrem ADFGVX. Rozszyfrowanie wiadomości umożliwiło historykom uzyskanie nowych informacji na temat wydarzeń na froncie wschodnim w ostatniej części I wojny światowej.

• Wyzwanie Double Transposition Challenge z 2007 roku autorstwa Klausa Schmeha i Otto Leibericha.

• Wyzwanie Hagelin M-209 Cipher Challenge z 2012 roku autorstwa Jean-François Bouchaudy.

• Wyzwania szyfru Hagelin M-209 z 1977 roku autorstwa Roberta Morrisa, Grega Mellena i Wayne'a Barkera.

• Wyzwanie Chaocipher Exhibit 6 Challenge autorstwa Johna Byrne'a, Ciphra Deavoursa i Louisa Kruha .

• Wygranie międzynarodowego konkursu Enigma zorganizowanego w 2015 roku w pamięci osiągnięć polskich matematyków, obejmującego odzyskanie ustawień klucza na podstawie niewielkiej liczby podwójnych wskaźników.

# 1.7 Struktura pracy

Ta praca dyplomowa jest podzielona następująco: W rozdziale 2 przedstawiamy ogólny zarys o stochastycznym przeszukiwaniu lokalnym. W rozdziale 3 prezentujemy zastosowanie metaheurystyk przeszukiwania lokalnego do kryptografii klasycznych szyfrów, w tym istotne dotychczasowe prace. W rozdziale 4 opisujemy nową metodologię skutecznego łamania klasycznych szyfrów przy użyciu metaheurystyk przeszukiwania lokalnego. Następnie przedstawiamy serię studiów przypadku, każdy w oddzielnym rozdziale, ilustrujących zastosowanie nowej metodologii do wyzwań kryptografii klasycznej, każde zawierające opis systemu szyfrowania, dotychczasowych prac, nowych ataków opartych na metodologii oraz ich oceny. Ostatni rozdział podsumowuje wyniki badań i zawiera sugestie dla dalszych badań.

# Stochastyczne przeszukiwanie lokalne

Rozdział ten przedstawia tło aplikacji algorytmów stochastycznego przeszukiwania lokalnego (SLS) do problemów kombinatorycznych. Rozpoczynamy od wprowadzenia do problemów kombinatorycznych. Następnie opisujemy algorytmy przeszukiwania, które mogą być zastosowane do ich rozwiązania. Następnie skupiamy się na konkretnej rodzinie metaheurystyk przeszukiwania, czyli stochastycznym przeszukiwaniu lokalnym, które jest istotne dla kryptologii klasycznych szyfrów. Wyczerpujący przegląd stochastycznego przeszukiwania lokalnego i jego zastosowań można znaleźć w książce „Stochastic Local Search: Foundations and Applications” z 2004 roku autorstwa Hoosa i Stutzle. Niniejszy rozdział stanowi podsumowanie głównych pojęć i ich definicji, przedstawionych w książce, rozszerzone o przykłady i zastosowania z dziedziny kryptologii.

# 2.1 Problemy kombinatoryczne

Problemy kombinatoryczne pojawiają się w wielu obszarach informatyki. Obejmują one zadania takie jak znajdowanie najkrótszych tras w grafach, planowanie i przydział zasobów. Kryptoanaliza klasycznych szyfrów również może być postrzegana jako problem kombinatoryczny. Rozwiązanie tych problemów polega na znalezieniu kolejności lub przyporządkowania zestawu obiektów, które spełniają określone warunki. Dla problemu planowania, poszczególne obiekty mogą być wydarzeniami do zaplanowania, a ich wartościami mogą być czasy, w których dane wydarzenie występuje. Dla większości problemów optymalizacji kombinatorycznej, przestrzeń potencjalnych rozwiązań dla danego przypadku problemu jest wykładnicza w stosunku do rozmiaru tego przypadku. Dla niektórych problemów kryptoanalizy, takich jak szyfr transpozycji kolumnowej, rozmiar problemu jest wykładniczy w stosunku do długości klucza (choć złożoność problemu zależy również od innych czynników, takich jak kompletny lub niekompletny prostokąt transpozycji). W większości problemów kryptoanalizy jednak rozmiar klucza jest stały, a rozmiar problemu jest również stały, choć zwykle bardzo duży.

Hoos i Stutzle rozróżniają między problemami a instancjami problemów. Problem jest problemem ogólnym lub abstrakcyjnym, takim jak "dla każdego danego zestawu punktów na płaszczyźnie, znajdź najkrótszą trasę łączącą te punkty". Instancją tego (ogólnego) problemu byłoby znalezienie najkrótszej trasy łączącej konkretne punkty na płaszczyźnie. Rozwiązanie takiej instancji problemu byłoby konkretnej najkrótszej trasie łączącej dany konkretny zestaw punktów. Rozwiązaniem problemu ogólnego jest jednak metoda lub algorytm, który dla każdej instancji problemu określa rozwiązanie tej instancji. Z kolei w kryptologii problemem ogólnym może być, na przykład, "znalezienie klucza szyfrującego, który przekształca tekst jawnie czytany w tekst zaszyfrowany, z użyciem znanego algorytmu szyfrowania". Instancją tego problemu byłoby znalezienie klucza szyfrującego dla konkretnej wiadomości zaszyfrowanej z użyciem znanego algorytmu szyfrowania i znanego klucza. Rozwiązaniem generycznego problemu jest jednak metoda lub algorytm, który dla każdej instancji problemu określa rozwiązanie dla tej instancji. Generycznym problemem związanym z kryptologią może być, w odniesieniu do konkretnego systemu szyfrowania, znalezienie klucza szyfrowania, podając dowolną parę tekstu jawnego i odpowiadającego mu tekstu zaszyfrowanego (tzw. "ataki znanego tekstu jawnego"). Instancją tego problemu byłoby znalezienie klucza dla konkretnej pary tekstu jawnego i zaszyfrowanego. W przypadku problemów kombinatorycznych wyróżnia się także rozwiązania kandydujące i rozwiązania. Rozwiązania kandydujące to potencjalne rozwiązania, które mogą pojawić się podczas próby rozwiązania danego problemu; ale w odróżnieniu od rozwiązań, nie muszą one spełniać wszystkich warunków z definicji problemu. W naszym przykładzie problemu kryptologicznego, każdy poprawny klucz szyfrowania byłby rozwiązaniem kandydującym, podczas gdy tylko te klucze kandydujące, które generują dany tekst zaszyfrowany (podczas szyfrowania danego tekstu jawnego), kwalifikują się jako rozwiązania.

Wiele problemów kombinatorycznych można określić jako problemy decyzyjne: dla nich rozwiązania
danej instancji są określone przez zestaw warunków logicznych. Dany graf i liczba kolorów, problem
znalezienia przypisania kolorów do jego wierzchołków w taki sposób, aby dwa połączone krawędzią wierzchołki nie miały przypisanych takich samych kolorów - problem kolorowania grafu - to przykład problemu decyzyjnego kombinatorycznego. Inne ważne problemy decyzyjne kombinatoryczne obejmują znalezienie spełniających przypisań prawdziwości dla danej formuły zdaniowej, problem spełnialności (SAT), lub planowanie serii wydarzeń tak, aby spełnione były określone ograniczenia priorytetowe. Nasz problem związany z kryptologią, polegający na znalezieniu klucza, który produkuje dany tekst zaszyfrowany, gdy zastosowany na danym tekście jawnym, jest również problemem decyzyjnym kombinatorycznym (i niektóre z tych problemów mogą być również modelowane jako problemy SAT).

Hoos i Stutzle również rozróżniają dwie wersje problemów decyzyjnych: wersję poszukiwań,
gdzie celem jest, dla danej instancji problemu, znalezienie rozwiązania (lub stwierdzenie, że nie istnieje rozwiązanie); wersję decyzyjną, w której dla danej instancji problemu chcemy odpowiedzieć na pytanie, czy istnieje rozwiązanie czy też nie. Te wersje są ze sobą blisko powiązane, ponieważ algorytmy rozwiązujące wersję poszukiwań mogą zawsze być użyte do rozwiązania wersji decyzyjnej. Ciekawym faktem jest, że dla wielu problemów decyzyjnych kombinatorycznych, zachodzi również odwrotna zależność: algorytmy rozwiązujące wersję decyzyjną problemu mogą być użyte do znajdowania rzeczywistych rozwiązań. W przypadku problemów kryptologicznych zazwyczaj interesuje nas poszukiwanie rozwiązań (kluczy), a nie odpowiedź na pytanie, czy rozwiązanie istnieje.

Inne problemy kombinatoryczne to problemy optymalizacyjne, a nie decyzyjne. Problemy optymalizacyjne można traktować jako uogólnienie problemów decyzyjnych, gdzie rozwiązania są dodatkowo oceniane przez funkcję celu, a celem jest znalezienie rozwiązań optymalnych względem wartości tej funkcji celu. Dla wcześniej wymienionego problemu kolorowania grafów, istnieje naturalna wersja optymalizacyjna, gdzie używa się zmiennej liczby kolorów, a celem jest dla danego grafu znalezienie kolorowania wierzchołków tak, aby spełnić warunki jak wyżej, używając minimalnej (a nie ustalonej) liczby kolorów. Każdy problem optymalizacyjny kombinatoryczny może być sformułowany jako problem maksymalizacji lub minimalizacji, gdzie często jedno z dwóch sformułowań jest bardziej naturalne. Algorytmicznie, problemy maksymalizacji i minimalizacji są traktowane równoważnie. W przypadku optymalizacji kombinatorycznej celem jest znalezienie kandydata na rozwiązanie o minimalnej (lub maksymalnej, odpowiednio) wartości funkcji celu. Alternatywnym celem może być znalezienie kandydata na rozwiązanie, którego wartość funkcji celu jest mniejsza lub równa pewnej wartości (dla problemów minimalizacyjnych) lub większa lub równa (dla problemów maksymalizacyjnych). Problem kryptoanalizy polegający na znalezieniu klucza, gdy znany jest tylko szyfrogram (tzw. atak "tylko-szyfrogram"), jest również problemem optymalizacyjnym kombinatorycznym. Celem jest znalezienie klucza, który po deszyfrowaniu danego szyfrogramu reprodukuje najbardziej "wiarygodny tekst jawnny".

Dwie szczególnie interesujące klasy problemów to P, klasa problemów, które mogą być rozwiązane przez deterministyczną maszynę w czasie wielomianowym, oraz NP, klasa problemów, które mogą być rozwiązane przez niedeterministyczną maszynę w czasie wielomianowym. Pytanie, czy P = NP, są  dwa szczególnie interesujące klasy problemów to P, klasa problemów, które można rozwiązać przy użyciu deterministycznej maszyny w czasie wielomianowym, oraz NP, klasa problemów, które można rozwiązać przy użyciu niedeterministycznej maszyny w czasie wielomianowym. Pytanie, czy P = NP, jest jednym z najważniejszych nierozwiązanych problemów w informatyce. Ponieważ wiele ważnych problemów z praktycznymi zastosowaniami należy do klasy NP, ale nie znane jest żadne deterministyczne algorytmy o czasie wielomianowym, ten problem nie jest tylko interesujący teoretycznie. Dla tych problemów, najlepsze znane algorytmy mają złożoność czasową wykładniczą. Dlatego dla rosnącej wielkości problemu, instancje problemów szybko stają się nieprzetwarzalne, nawet z ogromną mocą obliczeniową. Wiele z tych trudnych problemów z klasy NP jest ze sobą ściśle powiązanych i może być tłumaczonych na siebie nawzajem. Problem, który jest przynajmniej tak trudny jak jakikolwiek inny problem z NP (w sensie, że każdy problem z NP może być do niego zredukowany) jest nazywany NP-trudnym. Problemy NP-trudne można traktować jako przynajmniej tak trudne jak każdy problem w NP. Ale nie muszą one koniecznie należeć do klasy NP, ponieważ ich złożoność może być wyższa. Problemy NP-trudne, które zawierają się w NP, nazywane są NP-zupełnymi; te problemy są najtrudniejszymi problemami w NP. Problem SAT, problem komiwojażera (TSP), jak również wiele innych dobrze znanych problemów kombinatorycznych, w tym problem kolorowania grafów, problem plecaka, wiele problemów harmonogramowania, są NP-trudne lub NP-zupełne. Wystarczy znaleźć algorytm deterministyczny o czasie wielomianowym dla jednego tylko problemu NP-zupełnego, aby udowodnić, że P = NP. Dziś większość informatyków uważa, że P = NP.

Choć wiele problemów kombinatorycznych jest NP-trudnych, należy zauważyć, że nie każde zadanie obliczeniowe, które może zostać sformułowane jako problem kombinatoryczny, jest inherentnie trudne. Dobrze znanym przykładem problemu, który na pierwszy rzut oka może wymagać przeszukiwania wykładniczo dużego zbioru potencjalnych rozwiązań, jest problem najkrótszej ścieżki. Proste rekurencyjne narzędzie znane jako algorytm Dijkstry, może znaleźć najkrótsze ścieżki w czasie kwadratowym względem liczby wierzchołków w danym grafie. Chociaż problem może być NP-trudny, podklasa tego samego problemu może nie być. Na przykład, problem SAT dla formuł 2-CNF jest rozwiązywalny wielomianowo.

Innym podejściem do poszukiwania rozwiązań problemów kombinatorycznych jest akceptowanie suboptymalnych kandydatów na rozwiązania zamiast próby znalezienia tylko optymalnych rozwiązań. W ten sposób w wielu przypadkach złożoność obliczeniowa problemu może być wystarczająco zredukowana, aby problem mógł być praktycznie rozwiązywalny. W niektórych przypadkach pozwalanie na stosunkowo małą różnicę od rozwiązania optymalnego pozwala na deterministyczne rozwiązanie problemu w czasie wielomianowym. Czasem jednak nawet stosunkowo wydajne metody przybliżone nie mogą być opracowane lub problem jest problemem decyzyjnym, do którego pojęcie przybliżenia w ogóle nie może być stosowane. W takich przypadkach kolejną opcją jest rozważenie algorytmów probabilistycznych zamiast deterministycznych, jak opisano w kolejnych sekcjach.

# 2.2 Algorytmy Przeszukiwania

W tej sekcji przedstawimy kilka kluczowych pojęć, charakterystycznych strategii i kompromisów związanych z implementacją algorytmów przeszukiwania w celu rozwiązania trudnych problemów kombinatorycznych.

1 W pewnym stopniu podjęto próby budowania nowoczesnych systemów kryptograficznych opierających się na trudności rozwiązania problemów NP-zupełnych. Te wysiłki nie odniosły jednak sukcesu. Jednym z powodów jest to, że te problemy są uproszczoną wersją ogólnego problemu (np. plecak w kryptosystemie Merkle'a-Hellmana). Drugim argumentem jest to, że NP-zupełność odnosi się do najgorszego przypadku problemu, podczas gdy bezpieczeństwo kryptograficzne jest wymagane dla każdego przypadku. W badaniach literaturowych przeprowadzonych dla tej pracy magisterskiej, autor nie znalazł żadnej wcześniejszej pracy, która wykazałaby, że określony problem kryptoanalityczny może zostać odwzorowany na inny problem NP-zupełny.

# 2.2.1 Algorytmy wyszukiwania dla trudnych problemów kombinatorycznych

Podstawowo, wszystkie podejścia obliczeniowe do rozwiązywania trudnych problemów kombinatorycznych można określić jako algorytmy przeszukiwania. Podstawową ideą podejścia przeszukiwania jest iteracyjne generowanie i ocenianie kandydatów na rozwiązania; w przypadku decyzyjnych problemów kombinatorycznych ocena kandydata oznacza decyzję, czy jest to rzeczywiste rozwiązanie, podczas gdy w przypadku problemów optymalizacyjnych odpowiada to określeniu wartości funkcji celu. Choć czasowo złożoność poszukiwania rozwiązań dla NP-trudnych problemów kombinatorycznych może wzrastać wykładniczo wraz ze wzrostem rozmiaru instancji, ocena kandydatów na rozwiązania często może być znacznie bardziej efektywna, tj. w czasie wielomianowym. Na przykład dla danej instancji TSP, kandydatem na rozwiązanie odpowiadałaby wycieczka obejmująca każdy wierzchołek danej grafu dokładnie raz, a wartość funkcji celu może być łatwo obliczona przez zsumowanie wag wszystkich krawędzi użytych do tej wycieczki.

# 2.2.2 Algorytmy Przeszukiwania Perturbacyjnego vs. Konstrukcyjnego

Typowo, kandydackie rozwiązania dla instancji problemów kombinatorycznych składają się z atomowych przypisań wartości do obiektów, takich jak przypisanie wartości prawdy do indywidualnych zmiennych zdaniowych w przypadku SAT lub w kryptologii, przypisanie wartości do indywidualnych elementów klucza. Istniejące kandydackie rozwiązania mogą łatwo zostać przekształcone w nowe kandydackie rozwiązania przez zmodyfikowanie jednego lub więcej odpowiadających atomowych przypisań. Można to scharakteryzować jako zakłócenie danego kandydackiego rozwiązania. Hoos i Stutzle klasyfikują algorytmy przeszukiwania, które polegają na tym mechanizmie generowania kandydackich rozwiązań do przetestowania, jako metody przeszukiwania perturbacyjnego. W przypadku zastosowania do SAT, przeszukiwanie perturbacyjne zaczynałoby się od jednego lub więcej kompletnych przypisań prawdy, a następnie w każdym kroku generowane byłyby inne przypisania prawdy poprzez zmianę wartości logicznej liczby zmiennych w każdym takim przypisaniu. W przypadku kryptologii i szyfrów transpozycyjnych kolumnowych (patrz rozdział 5), przeszukiwanie perturbacyjne zaczynałoby się od losowego klucza transpozycji, a w każdym kroku generowane byłyby nowe klucze transpozycji przez zamianę zawartości dowolnych dwóch elementów klucza.

Podczas gdy dla podejść perturbacyjnych wyszukiwanie odbywa się typowo bezpośrednio w przestrzeni rozwiązań kandydujących, czasami przydatne jest także uwzględnienie częściowych rozwiązań kandydujących w przestrzeni poszukiwań, czyli rozwiązań kandydujących, dla których niektóre przypisania atomowe nie są określone. Przykładem takich częściowych przypisań jest częściowa trasa wycieczki dla instancji problemu komiwojażera, która odpowiada ścieżkom w odpowiadającym grafie, odwiedzających podzbiór wierzchołków i może być rozbudowana do pełnego cyklu przez dodanie dodatkowych krawędzi. Algorytmy rozwiązywania tego typu problemów nazywane są metodami konstrukcyjnymi lub heurystykami konstrukcyjnymi. Przykładem konstrukcyjnego przeszukiwania dla kryptoanalizy jest odzyskiwanie ustawień plugboard Enigmy (patrz sekcja 3.5.1). Dla niektórych instancji problemów wiadomo, że poprawne rozwiązanie powinno zawierać dokładnie 10 połączeń plugboard. Wyszukiwanie może zacząć się od braku połączeń, a następnie stopniowo dodawać, wymieniać lub usuwać połączenia, aż zostanie znaleziony optymalny zestaw 10 połączeń. Oznacza to, że przestrzeń poszukiwań obejmuje również częściowe rozwiązania z mniej niż 10 połączeniami.

# 2.2.3 Systematyczne vs. Lokalne Przeszukiwanie

Hoos i Stutzle dokonują również rozróżnienia między systematycznym a lokalnym przeszukiwaniem: Systematyczne algorytmy przeszukiwania przechodzą przez przestrzeń poszukiwań danego problemu w sposób systematyczny, co gwarantuje, że ostatecznie zostanie znalezione rozwiązanie, lub jeśli nie istnieje, zostanie to stwierdzone z pewnością. Typową właściwością algorytmów opartych na systematycznym przeszukiwaniu jest kompletność. Odpowiednikiem systematycznego przeszukiwania w kryptologii jest "brute force", w którym wszystkie poprawne klucze w przestrzeni kluczy są systematycznie badane w celu znalezienia optymalnych rozwiązań (oryginalnego klucza szyfrowania).

Jedną z metod systematycznego badania wszystkich możliwych rozwiązań jest backtracking. Przy użyciu backtrackingu przestrzeń możliwych kompletnych rozwiązań reprezentowana jest przez liście drzewa. Pozostałe węzły drzewa reprezentują rozwiązania częściowe. Zacznijmy od korzenia drzewa, wybierając pierwsze dziecko, a następnie kontynuujmy od tego momentu aż do osiągnięcia końca drzewa, wtedy mamy kompletny wynik, który oceniamy. Następnie wracamy, przechodząc do poprzedniego węzła i wybierając kolejne dziecko, które nie zostało odwiedzone, i powtarzamy ten proces od tego dziecka. Zatrzymujemy się, gdy wszystkie węzły i liście (kompletne rozwiązania) zostaną odwiedzone. Takie algorytmy backtrackingu mają tendencję do działać wykładniczo. Na szczęście często możliwe jest przycięcie niektórych gałęzi drzewa (poddrzew), bez konieczności odwiedzania ich węzłów. Taki podejście jest powszechnie znane jako branch & bound. Z kolei algorytmy lokalnego przeszukiwania zaczynają od pewnej lokalizacji przestrzeni poszukiwań i następnie przemieszczają się od bieżącej lokalizacji do sąsiedniej lokalizacji w przestrzeni poszukiwań, gdzie każda lokalizacja ma tylko ograniczoną liczbę sąsiadów, a każdy ruch jest określany na podstawie lokalnej wiedzy. Typowo algorytmy lokalnego przeszukiwania są niekompletne, to znaczy nie ma gwarancji, że istniejące rozwiązanie zostanie ostatecznie znalezione, a fakt, że nie istnieje rozwiązanie, nigdy nie może być określony z pewnością. Ponadto, metody lokalnego przeszukiwania mogą odwiedzić tę samą lokalizację w przestrzeni poszukiwań więcej niż raz. W rzeczywistości wiele algorytmów lokalnego przeszukiwania ma tendencję do utknięcia w pewnej części przestrzeni poszukiwań, z której nie mogą się wydostać bez specjalnych mechanizmów, takich jak kompletny restart procesu wyszukiwania lub inne kroki dywersyfikacji. Reszta tego rozdziału skupia się na metodach i algorytmach lokalnego przeszukiwania.

# 2.3 Losowy przeszukiwanie lokalne

Wiele dobrze znanych i wydajnych algorytmów przeszukiwania lokalnego wykorzystuje losowe wybory w generowaniu lub wyborze potencjalnych rozwiązań dla danego problemu kombinatorycznego. Te algorytmy nazywają się algorytmami losowego przeszukiwania lokalnego i stanowią jedno z najskuteczniejszych i najczęściej stosowanych podejść do rozwiązywania trudnych problemów kombinatorycznych, takich jak problem komiwojażera. Jak opisujemy w kolejnych rozdziałach, zostały one również szeroko wykorzystane w kryptoanalizie klasycznych szyfrów.
W tej sekcji opisujemy strukturę generycznego podejścia losowego przeszukiwania lokalnego oraz jego komponentów.

# 2.3.1 Przegląd losowego przeszukiwania lokalnego

Algorytmy przeszukiwania lokalnego zwykle działają w następujący sposób. Dla danego przypadku problemu kombinatorycznego, poszukiwanie rozwiązań odbywa się w przestrzeni potencjalnych rozwiązań, nazywanej również przestrzenią poszukiwań. Należy zauważyć, że przestrzeń poszukiwań może obejmować częściowe rozwiązania kandydujące, w kontekście algorytmów konstrukcyjnych. Proces przeszukiwania lokalnego rozpoczyna się od wyboru początkowego rozwiązania kandydującego, a następnie przebiega iteracyjnie poprzez przemieszczanie się od jednego rozwiązania kandydującego do sąsiedniego rozwiązania kandydującego, gdzie decyzja o każdym kroku przeszukiwania opiera się na ograniczonej liczbie informacji o sąsiedztwie aktualnego rozwiązania kandydującego.
